acf(ret)
rm(list=ls()) # clear
library("ggfortify")
library("quantmod")
library("tseries")
library("moments")
### get the SMI data directly from yahoo finance
MyData   <- getSymbols("AMZN", auto.assign=FALSE, from="1990-01-01", src='yahoo')
head(MyData)
#### important NOTE MyData comes as an xts object
#### an extensible ts object. in short it is a matrix with a time index !
is.xts(MyData)
price = MyData$AMZN.Close
idx   = !is.na(price)
price = price[idx]
autoplot(price)
## TASK 1:
# now compute log returns
# delete the "na" (for later purposes)
# plot returns
# now plot returns
ret <- diff(log(price))
idx   = !is.na(ret)
ret = ret[idx]
autoplot(ret) + labs(title="AMZN",
subtitle="Returns",
x ="time") +
theme(panel.grid.minor = element_blank())
##       y =expression(Delta),
## TASK 2
# Compute descriptive statistics on ret:
# use functions from the library 'moments'
# some descriptive statistics on ret:
sumStat <- c(length(ret), min(ret), max(ret), mean(ret), median(ret), sqrt(var(ret)), skewness(ret), kurtosis(ret)  )
print("# elements, min, max, mean, median, std, skew, kurtosis (not excess kurtosis)", quote = FALSE)
cat(sumStat)
# we now compare against normality
# plot a histogram of returns and compare with the a density generated
# from normal variates with same mean and variance:
hist(ret,
100,
prob = TRUE,
col="gray", main = "Histogram", xlab="AMZN returns")
lines( sort(as.numeric(ret)), dnorm(sort(as.numeric(ret)), mean = mean(as.numeric(ret)),
sd = sqrt(var(as.numeric(ret)))),
lwd = 2, col = "red"
)
## TASK 3 do a qqplot on returns
## use qqnorm
qqnorm(ret)
qqline(ret, col=2)
## TASK 4:
# do a Jarque Bera test to test for normality
# use the function jarque.bera.test from the package 'tseries'
jarque.bera.test(ret)
## TASK 5:
# Compute unconditional (daily) volatility
# Compute unconditional monthly, annualized volatility
sd(ret)
sd(ret*sqrt(21)) #monthly
sd(ret*sqrt(252))#yearly
sd(ret)*sqrt(252)
## TASK 6:
# Compute rolling window 30d volatiliy
# Compute eqma  volatiliy
ret2   <- as.numeric(ret^2) ## need to convert to numeric because otherwise loop below fails
N      <- length(ret)
vol30d <- numeric(N)
lambda <-  0.94;
ewma   <- numeric(N)
for (i in 31:N){
vol30d[i] <- sqrt(var(ret[(i-30):i]))
tmp <- ret2[i:1];
ewma[i] <- (1-lambda)*sum((lambda^(0:(i-1)))*tmp)
}
n <- length(ret2)  #gpt version do not trust
vol <- rep(NA,n)
for (i in 30:n) {
vol[i] <- sqrt(30)*sd(ret2[(i-29):i])
}
## compute a for loop here which fills vol30d and ewma:
## ewma has variance unit; convert ewma to vol !
tmp     <- xts(order.by=index(ret))
allVola <- merge(tmp, vol30d=vol30d)
allVola <- merge(allVola, ewma=ewma)
autoplot(allVola[31:N])
## check how different ewma is vis-a-vis vol30d
autoplot(allVola[,1]-allVola[,2])
acf(ret)
acf(log(ret))
acf(ret^2)
acf(abs(ret))
library("fGarch")
install.packages("fGarch")
library("fGarch")
condDist <- "norm"
q <- 1
gfit01 <- garchFit(formula = ~garch(1,0), data = ret, cond.dist = condDist)
summary(gfit01)
# close windows, clear variables
rm(list = ls(all = TRUE))
graphics.off()
# install and load packages
libraries = c("kernlab", "ellipse", "xtable")
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
install.packages(x)
})
lapply(libraries, library, quietly = TRUE, character.only = TRUE)
################################################################################
########################## manipulate subroutine specc #########################
#################### to return eigenvalues and eigenvectors ####################
################################################################################
setGeneric("specc",function(x, ...) standardGeneric("specc"))
setMethod("specc", signature(x = "formula"),
function(x, data = NULL, na.action = na.omit, ...)
{
mt = terms(x, data = data)
if(attr(mt, "response") > 0) stop("response not allowed in formula")
attr(mt, "intercept") = 0
cl                    = match.call()
mf                    = match.call(expand.dots = FALSE)
mf$formula            = mf$x
mf$...                = NULL
mf[[1]]               = as.name("model.frame")
mf                    = eval(mf, parent.frame())
na.act                = attr(mf, "na.action")
x                     = model.matrix(mt, mf)
res                   = specc(x, ...)
cl[[1]] = as.name("specc")
if(!is.null(na.act)) n.action(res) = na.action ;
return(res)
})
setMethod("specc", signature(x = "matrix"),
function(x, centers, kernel = "rbfdot", kpar = "automatic", nystrom.red = FALSE,
nystrom.sample = dim(x)[1]/6, iterations = 200, mod.sample = 0.75,
na.action = na.omit, ...)
{
x    = na.action(x)
rown = rownames(x)
x    = as.matrix(x)
m    = nrow(x)
if (missing(centers)) stop("centers must be a number or a matrix");
if (length(centers) == 1) {
nc =  centers
if (m < centers) stop("more cluster centers than data points.");
}
else nc = dim(centers)[2];
if(is.character(kpar)) {
kpar = match.arg(kpar, c("automatic", "local"))
if(kpar == "automatic") {
if (nystrom.red == TRUE) {
sam = sample(1:m, floor(mod.sample * nystrom.sample))
}else {
sam = sample(1:m, floor(mod.sample * m))
}
sx   = unique(x[sam, ])
ns   = dim(sx)[1]
dota = rowSums(sx * sx) / 2
ktmp = crossprod(t(sx))
for (i in 1:ns) ktmp[i, ] = 2 * (-ktmp[i, ] + dota + rep(dota[i], ns));
## fix numerical prob.
ktmp[ktmp < 0] = 0
ktmp           = sqrt(ktmp)
kmax           = max(ktmp)
kmin           = min(ktmp + diag(rep(Inf,dim(ktmp)[1])))
kmea           = mean(ktmp)
lsmin          = log2(kmin)
lsmax          = log2(kmax)
midmax         = min(c(2 * kmea, kmax))
midmin         = max(c(kmea/2, kmin))
rtmp           = c(seq(midmin, 0.9 * kmea, 0.05 * kmea),
seq(kmea, midmax, 0.08 * kmea))
if ((lsmax - (Re(log2(midmax)) + 0.5)) < 0.5) {
step = (lsmax - (Re(log2(midmax)) + 0.5))
} else {
step = 0.5
}
if (((Re(log2(midmin))-0.5)-lsmin) < 0.5 ) {
stepm = ((Re(log2(midmin)) - 0.5) - lsmin)
} else {
stepm = 0.5
}
tmpsig = c(2^(seq(lsmin, (Re(log2(midmin)) - 0.5), stepm)),
rtmp, 2^(seq(Re(log2(midmax)) + 0.5, lsmax,step)))
diss   = matrix(rep(Inf,length(tmpsig) * nc), ncol = nc)
for (i in 1:length(tmpsig)){
ka       = exp((-(ktmp^2)) / (2 * (tmpsig[i]^2)))
diag(ka) = 0
d        = 1 / sqrt(rowSums(ka))
if(!any(d == Inf) && !any(is.na(d)) && (max(d)[1] - min(d)[1] < 10^4)) {
l         = d * ka %*% diag(d)
xi        = eigen(l, symmetric = TRUE)$vectors[, 1:nc]
yi        = xi / sqrt(rowSums(xi^2))
res       = kmeans(yi, centers, iterations)
diss[i, ] = res$withinss
}
}
ms     = which.min(rowSums(diss))
kernel = rbfdot((tmpsig[ms]^(-2))/2)
## Compute Affinity Matrix
if (nystrom.red == FALSE) km = kernelMatrix(kernel, x);
}
if (kpar=="local") {
if (nystrom.red == TRUE) {
stop ("Local Scaling not supported for nystrom reduction.")
}
s    = rep(0, m)
dota = rowSums(x * x) / 2
dis  = crossprod(t(x))
for (i in 1:m) dis[i, ]= 2 * (-dis[i, ] + dota + rep(dota[i], m));
## fix numerical prob.
dis[dis < 0] = 0
for (i in 1:m) s[i] = median(sort(sqrt(dis[i, ]))[1:5]);
## Compute Affinity Matrix
km     = exp(-dis / s%*%t(s))
kernel = "Localy scaled RBF kernel"
}
} else {
if(!is(kernel, "kernel")) {
if(is(kernel, "function")) kernel = deparse(substitute(kernel));
kernel = do.call(kernel, kpar)
}
if(!is(kernel, "kernel")) stop("kernel must inherit from class `kernel'");
## Compute Affinity Matrix
if (nystrom.red == FALSE) km = kernelMatrix(kernel, x);
}
if (nystrom.red == TRUE){
n      = floor(nystrom.sample)
ind    = sample(1:m, m)
x      = x[ind, ]
tmps   = sort(ind, index.return = TRUE)
reind  = tmps$ix
A      = kernelMatrix(kernel, x[1:n, ])
B      = kernelMatrix(kernel, x[-(1:n), ], x[1:n, ])
d1     = colSums(rbind(A, B))
d2     = rowSums(B) + drop(matrix(colSums(B), 1) %*% .ginv(A) %*% t(B))
dhat   = sqrt(1 / c(d1, d2))
A      = A * (dhat[1:n] %*% t(dhat[1:n]))
B      = B * (dhat[(n + 1):m] %*% t(dhat[1:n]))
Asi    = .sqrtm(.ginv(A))
Q      = A + Asi %*% crossprod(B) %*% Asi
tmpres = svd(Q)
U      = tmpres$u
L      = tmpres$d
V      = rbind(A, B) %*% Asi %*% U %*% .ginv(sqrt(diag(L)))
yi     = matrix(0, m, nc)
# for(i in 2:(nc +1)) yi[,i-1] = V[,i]/V[,1];
## specc
for(i in 1:nc) yi[, i] = V[, i]/sqrt(sum(V[, i]^2));
res = kmeans(yi[reind, ], centers, iterations)
} else {
if(is(kernel)[1] == "rbfkernel") diag(km) = 0;
d   = 1 / sqrt(rowSums(km))
l   = d * km %*% diag(d)
xu  = eigen(l)$values[1:nc]
xi  = eigen(l)$vectors[, 1:nc]
xxu = eigen(l)$values
xxi = eigen(l)$vectors
yi  = xi / sqrt(rowSums(xi^2))
res = kmeans(yi, centers, iterations)
}
ll                 = function(l) colMeans(x[which(res$cluster==l), ])
cent               = matrix(unlist(lapply(1:nc, ll)), ncol = dim(x)[2], byrow = TRUE)
ll                 = function(l) sum((x[which(res$cluster == l),] - cent[l, ])^2);
withss             = unlist(lapply(1:nc, ll))
names(res$cluster) = rown
return(new("specc", .Data = list(data = res$cluster, evalues = xxu, evectors = xxi),
size = res$size, centers = cent, withinss = withss, kernelf = kernel))
})
###########################################################################
############################## main computation ###########################
###########################################################################
set.seed(1)
# define eight points
eight   = cbind(c(-3, -2, -2, -2, 1, 1, 2, 4), c(0, 4, -1, -2, 4, 2, -4, -3))
eight   = eight[c(8, 7, 3, 1, 4, 2, 6, 5), ]
sc      = specc(eight, centers = 2)
centers = attr(sc, "centers") # center coordinates
size    = attr(sc, "size")    # size of clusters
datacl  = sc$data             # clusters
evalues = sc$evalues          # eigenvalues
evectors= sc$evectors         # eigenvectors
# Latex export
xtable(as.matrix(evalues))
xtable(evectors)
# final output
plot(eight, type = "n", xlab = "price conciousness", ylab = "brand loyalty",
xlim = c(-4, 4), main = "8 points")
points(eight, pch = 21, cex = 2.7, bg = "white")
text(eight, as.character(1:8), col = "red3", xlab = "first coordinate",
ylab = "second coordinate", main = "8 points", cex = 1)
lines(ellipse(0.6, centre = centers[2, ], scale = c(1.2, 2)), col = "red3", lwd = 2)
lines(ellipse(0.6, centre = centers[1, ], scale = c(.7, .4)), col = "blue3",lwd = 2)
eight
# clear all variables
rm(list = ls(all = TRUE))
graphics.off()
# Drug data
zi = rbind(c(1, 0, 1, 0, 1, 0, 0, 0, 0, 21), c(1, 0, 1, 0, 0, 1, 0, 0, 0, 32), c(1,
0, 1, 0, 0, 0, 1, 0, 0, 70), c(1, 0, 1, 0, 0, 0, 0, 1, 0, 43), c(1, 0, 1, 0,
0, 0, 0, 0, 1, 19), c(1, 0, 0, 1, 1, 0, 0, 0, 0, 683), c(1, 0, 0, 1, 0, 1, 0,
0, 0, 596), c(1, 0, 0, 1, 0, 0, 1, 0, 0, 705), c(1, 0, 0, 1, 0, 0, 0, 1, 0, 295),
c(1, 0, 0, 1, 0, 0, 0, 0, 1, 99), c(0, 1, 1, 0, 1, 0, 0, 0, 0, 46), c(0, 1, 1,
0, 0, 1, 0, 0, 0, 89), c(0, 1, 1, 0, 0, 0, 1, 0, 0, 169), c(0, 1, 1, 0, 0,
0, 0, 1, 0, 98), c(0, 1, 1, 0, 0, 0, 0, 0, 1, 51), c(0, 1, 0, 1, 1, 0, 0,
0, 0, 738), c(0, 1, 0, 1, 0, 1, 0, 0, 0, 700), c(0, 1, 0, 1, 0, 0, 1, 0,
0, 847), c(0, 1, 0, 1, 0, 0, 0, 1, 0, 336), c(0, 1, 0, 1, 0, 0, 0, 0, 1,
196))
y = zi[, 10]
# Design matrix
I = 2  # sex M - F
J = 2  # drug Yes - No
K = 5  # age category 16-29, 30-44, 45-64, 65-74, 75++
# Mean age per group: for Men and for Women
average = c(c(23.2, 36.5, 54.3, 69.2, 79.5), c(23.2, 36.5, 54.3, 69.2, 79.5))
X   = rbind(c(1, 1), c(1, 1), c(1, 1), c(1, 1), c(1, 1), c(1, -1), c(1, -1), c(1, -1),
c(1, -1), c(1, -1))
X1  = cbind(X, average)  # Xi=design matrix for group i=1,2
n   = dim(X1)
n1  = n[1]
n2  = n[2]
df  = n1 - n2
label = zi[, 3] == 1
n1jk  = y[label]  # nijk is the effective in each cell, i=1,2
label = zi[, 3] == 0
n2jk  = y[label]
b0 = 0 * rep(1, n2)  # current value of beta
# max likelihood in logistic models for 3-way contingency tables
ff = function(b0) {
-sum(n1jk * log(matrix(1, length(n1jk), 1)/(matrix(1, length(n1jk), 1) + exp(-X1 %*%
b0)))) - sum(n2jk * log(matrix(1, dim(matrix(1, length(n1jk), 1)/(matrix(1,
length(n1jk), 1) + exp(-X1 %*% b0)))) - matrix(1, length(n1jk), 1)/(matrix(1,
length(n1jk), 1) + exp(-X1 %*% b0))))
}
(b      = optim(b0, ff)$par)
loglik  = optim(b0, ff)$value
N       = sum(y)
p1      = matrix(1, length(n1jk), 1)/(matrix(1, length(n1jk), 1) + exp(-X1 %*% b))
p2      = matrix(1, length(n2jk), 1)/(matrix(1, length(n2jk), 1) + exp(X1 %*% b))
nfit    = rbind((n1jk + n2jk) * p1, (n1jk + n2jk) * p2)
nobs    = c(n1jk, n2jk)
e       = log(nobs) - log(nfit)
print("degree of freedom")
print(df)
(G2     = 2 * sum(nobs * e))
(pvalG2 = 1 - pchisq(G2, df))
(chi2   = sum(((nobs - nfit)^2)/nfit))
(pvalG2 = 1 - pchisq(G2, df))
print(" ")
print("  observed    fitted")
print("    values    values")
cbind(nobs, nfit)
print(" ")
oddratfit = log(p1/p2)
oddrat    = log(n1jk/n2jk)
plot(X1[1:K, 3], oddratfit[1:K], type = "l", ylim = c(-3.5, -0.5), ylab = "", xlab = "",
lwd = 2)
par(new = TRUE)
plot(X1[(K + 1):(2 * K), 3], oddratfit[(K + 1):(2 * K)], type = "l", ylim = c(-3.5,
-0.5), xlab = "Age category", ylab = "log of odds-ratios", lwd = 2)
points(X1[1:K, 3], oddrat[1:K], pch = "*", cex = 2, col = "red3")
points(X1[(K + 1):(2 * K), 3], oddrat[(K + 1):(2 * K)], cex = 2, col = "blue3")
title(paste("Fit of the log of the odds-ratios "))
# logistic model with curvature term: log(y)~gender + age + age^2
(X2 = cbind(X, average, average * average))
n   = dim(X2)
n1  = n[1]
n2  = n[2]
df2 = n1 - n2
label = zi[, 3] == 1
n1jk  = y[label]
label = zi[, 3] == 0
n2jk  = y[label]
b0 = 0 * rep(1, n2)
f2 = function(b0) {
-sum(n1jk * log(matrix(1, length(n1jk), 1)/(matrix(1, length(n1jk), 1) + exp(-X2 %*%
b0)))) - sum(n2jk * log(matrix(1, dim(matrix(1, length(n1jk), 1)/(matrix(1,
length(n1jk), 1) + exp(-X2 %*% b0)))) - matrix(1, length(n1jk), 1)/(matrix(1,
length(n1jk), 1) + exp(-X2 %*% b0))))
}
(b = optim(b0, f2)$par)
loglik  = optim(b0, f2)$value
N       = sum(y)
p1      = matrix(1, length(n1jk), 1)/(matrix(1, length(n1jk), 1) + exp(-X2 %*% b))
p2      = matrix(1, length(n2jk), 1)/(matrix(1, length(n2jk), 1) + exp(X2 %*% b))
nfit    = rbind((n1jk + n2jk) * p1, (n1jk + n2jk) * p2)
nobs    = c(n1jk, n2jk)
e       = log(nobs) - log(nfit)
print("degree of freedom")
print(df2)
(GG2      = 2 * sum(nobs * e))
(pvalGG2  = 1 - pchisq(GG2, df2))
(chi2     = sum(((nobs - nfit)^2)/nfit))
(pvalG2   = 1 - pchisq(GG2, df2))
print(" ")
print("  observed    fitted")
print("    values    values")
cbind(nobs, nfit)
print(" ")
oddratfit = log(p1/p2)
oddrat    = log(n1jk/n2jk)
dev.new()
plot(X2[1:K, 3], oddratfit[1:K], type = "l", ylim = c(-3.5, -0.5), ylab = "", xlab = "",
lwd = 2)
par(new = TRUE)
plot(X2[(K + 1):(2 * K), 3], oddratfit[(K + 1):(2 * K)], type = "l", ylim = c(-3.5,
-0.5), xlab = "Age category", ylab = "log of odds-ratios", lwd = 2)
points(X2[1:K, 3], oddrat[1:K], pch = "*", cex = 2, col = "red3")
points(X2[(K + 1):(2 * K), 3], oddrat[(K + 1):(2 * K)], cex = 2, col = "blue3")
title(paste("Fit of the log of the odds-ratios "))
# test model one against model two
print("degree of freedom")
print(df - df2)
(overallG2  = G2 - GG2)
(pvaloG2    = 1 - pchisq(overallG2, df - df2))
# clear all variables
rm(list = ls(all = TRUE))
graphics.off()
# load data
data = read.table("bostonh.dat")
setwd("C:/Users/timda/repo/mva/MVA/QID-1118-MVAnpcahousi")
# clear all variables
rm(list = ls(all = TRUE))
graphics.off()
# load data
data = read.table("bostonh.dat")
# transform data
xt       = data
xt[, 1]  = log(data[, 1])
xt[, 2]  = data[, 2]/10
xt[, 3]  = log(data[, 3])
xt[, 5]  = log(data[, 5])
xt[, 6]  = log(data[, 6])
xt[, 7]  = (data[, 7]^(2.5))/10000
xt[, 8]  = log(data[, 8])
xt[, 9]  = log(data[, 9])
xt[, 10] = log(data[, 10])
xt[, 11] = exp(0.4 * data[, 11])/1000
xt[, 12] = data[, 12]/100
xt[, 13] = sqrt(data[, 13])
xt[, 14] = log(as.numeric(data[, 14]))
data     = xt[, -4]
n1  = nrow(data)
n2  = ncol(data)
x   = (data - matrix(mean(as.matrix(data)), n1, n2, byrow = T))/matrix(sqrt((n1 - 1) *
apply(data, 2, var)/n1), n1, n2, byrow = T)  # standardizes the data
eig = eigen((n1 - 1) * cov(x)/n1)  # spectral decomposition
e   = eig$values
v   = eig$vectors
x1  = as.matrix(x - matrix(mean(as.matrix(x)), nrow(x), ncol(x), byrow = T))
r1  = x1 %*% v
r   = cor(cbind(r1, x))
# correlations between variables and pc's
r12  = r[14:26, 1:2]
r13  = cbind(r[14:26, 1], r[14:26, 3])
r32  = cbind(r[14:26, 3], r[14:26, 2])
r123 = r[14:26, 1:3]
# plot
par(mfrow = c(2, 2))
ucircle = cbind(cos((0:360)/180 * pi), sin((0:360)/180 * pi))
plot(ucircle, type = "l", lty = "solid", col = "blue", xlab = "First PC", ylab = "Second PC",
main = "Boston Housing", cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.6, lwd = 2)
abline(h = 0, v = 0)
label = c("X1", "X2", "X3", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13",
"X14")
text(r12, label)
ucircle = cbind(cos((0:360)/180 * pi), sin((0:360)/180 * pi))
plot(ucircle, type = "l", lty = "solid", col = "blue", xlab = "Third PC", ylab = "Second PC",
main = "Boston Housing", cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.6, lwd = 2)
abline(h = 0, v = 0)
label = c("X1", "X2", "X3", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13",
"X14")
text(r32, label)
ucircle = cbind(cos((0:360)/180 * pi), sin((0:360)/180 * pi))
plot(ucircle, type = "l", lty = "solid", col = "blue", xlab = "First PC", ylab = "Third PC",
main = "Boston Housing", cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.6, lwd = 2)
abline(h = 0, v = 0)
label = c("X1", "X2", "X3", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13",
"X14")
text(r13, label)
ucircle = cbind(cos((0:360)/180 * pi), sin((0:360)/180 * pi))
plot(ucircle, type = "l", lty = "solid", col = "blue", xlab = "X", ylab = "Y", cex.lab = 1.2,
cex.axis = 1.2, lwd = 2)
abline(h = 0, v = 0)
label = c("X1", "X2", "X3", "X5", "X6", "X7", "X8", "X9", "X10", "X11", "X12", "X13",
"X14")
text(r123, label)
x
data
data
mean(as.matrix(data)), n1, n2, byrow = T)
matrix(mean(as.matrix(data)), n1, n2, byrow = T)
?mean
mean(data)
as.matrix(data)
mean(as.matrix(data))
matrix(mean(as.matrix(x)), nrow(x), ncol(x), byrow = T)
mean(as.matrix(x))
View(r)
data - matrix(mean(as.matrix(data)), n1, n2, byrow = T)
apply(data, 2, var)
sqrt((n1 - 1) *
apply(data, 2, var)/n1)
matrix(sqrt((n1 - 1) *
apply(data, 2, var)/n1), n1, n2, byrow = T)
View(x)
matrix(mean(as.matrix(x)), nrow(x), ncol(x), byrow = T)
x1
r1
v
x
r123
